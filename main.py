# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B6PhowWCXgnFxh_NNhDPkGcfqsqBuKMG
"""

pip install beautifulsoup4

pip show beautifulsoup4

pip install requests

import requests
#response = requests.get('https://www.brownells.com/')

from bs4 import BeautifulSoup
import pandas as pd
file1 = open('/content/brownells.txt', 'r') 
Lines = file1.readlines() 
Lines

#count = 0
# Strips the newline character 
#for line in Lines: 
    #print("Line{}: {}".format(count, line.strip()))

from requests.exceptions import HTTPError
for url in Lines:
    try:
        response = requests.get(url)

        # If the response was successful, no Exception will be raised
        response.raise_for_status()
    except HTTPError as http_err:
        print(f'HTTP error occurred: {http_err}')  # Python 3.6
    except Exception as err:
        print(f'Other error occurred: {err}')  # Python 3.6
    else:
        print('Success!')

HEADERS = ({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36'})

r = requests.get(Lines[7], headers=HEADERS)
r.status_code
#requests.get(Lines[6])

pip install lxml

File = open("out.csv", "a")

soup = BeautifulSoup(r.content, "lxml")
movie_containers = soup.find_all('div', id = 'wrap')
print(type(movie_containers))
print(len(movie_containers))

Product_title=[]
for i in movie_containers:
  product_title=i.find_all('span')[0:2]
  for j in product_title:
    print(j.text)
    Product_title.append(j.text)
Product=[]
title=[]
for i in range(len(Product_title)):
  if i%2==0:
    Product.append(Product_title[i])
  else :
    title.append(Product_title[i])

Stock_status=[]
for i in movie_containers:
  b= i['data-status']
  Stock_status.append(b)
Stock_status

Manufacturer=['Badger Ordnance']*20

list_of_tuples = list(zip(Product,title, Stock_status,Manufacturer))
df = pd.DataFrame(list_of_tuples, 
                  columns = ['Product','title', 'Stock_status','Manufacturer'])  
     
# Print data.  
df

df.to_csv('Output.csv', index=False)

# 'HTTP error occurred: 400 Client Error' showing for all urls except second and last on the txt file.
# I have done scrapping on only last url on txt file.
# all products have same hyperlink and no other individual hyperlink is there on the last url webpage.
# all have same manufacturer